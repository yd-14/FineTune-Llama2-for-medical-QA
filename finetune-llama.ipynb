{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7045374,"sourceType":"datasetVersion","datasetId":4054084},{"sourceId":7888360,"sourceType":"datasetVersion","datasetId":4631018},{"sourceId":4298,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3093}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Note: It is important to specify the versions of libraries used after getting the code to run first time. These libraries are updated quite often and this usually results in error and incompatibilty between liberaries.\n\nUse: \n\"pip freeze > requirements.txt\"\nto generate a file containing all the details about libraries used and their versions. This will come handy in case of replicating code.\n\nOn kaggle keep the environment pinned to original to avoid conflict in future","metadata":{}},{"cell_type":"code","source":"!pip install -q torch peft bitsandbytes transformers trl accelerate","metadata":{"execution":{"iopub.status.busy":"2024-03-20T20:15:30.017945Z","iopub.execute_input":"2024-03-20T20:15:30.018293Z","iopub.status.idle":"2024-03-20T20:15:48.157249Z","shell.execute_reply.started":"2024-03-20T20:15:30.018267Z","shell.execute_reply":"2024-03-20T20:15:48.156106Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-03-20T20:15:48.159172Z","iopub.execute_input":"2024-03-20T20:15:48.159480Z","iopub.status.idle":"2024-03-20T20:16:13.902235Z","shell.execute_reply.started":"2024-03-20T20:15:48.159454Z","shell.execute_reply":"2024-03-20T20:16:13.901214Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline\n)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-20T20:40:26.364413Z","iopub.execute_input":"2024-03-20T20:40:26.364822Z","iopub.status.idle":"2024-03-20T20:40:41.975355Z","shell.execute_reply.started":"2024-03-20T20:40:26.364790Z","shell.execute_reply":"2024-03-20T20:40:41.974487Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load csv file with QA data divide into train and test\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom datasets import Dataset\n\ndata_name = \"/kaggle/input/diabetes-qa/QA_dataset - Sheet1.csv\" #replace with file path\ndf = pd.read_csv(data_name)\n\n# last 40 kept as test data\n# first 40 for training\ntest_data = df.tail(40)\ntrain_data = df.head(len(df) - 40)\n\n# It is import to convert data into Dataset of Dataset library to be used with \n# finetuning trainer provided by Hugging face\ntrain_dataset = Dataset.from_pandas(train_data)","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Standard Procedure to load a LLM using Hugging Face libraries\n#llama-2 7b-chat is used for both tokenization and generation\n#To access llama-2 in kaggle go to +Add input -> Models -> llama-2\n#Copy it's path here as base_model_name\n#The model can also be accessed through a hugging face \n#After that it can be loaded on google colab through hugging face login\n\nbase_model_name = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n\n# The fine tuned model will be saved with this name\nrefined_model = \"llama-2-7b-medical-enhanced\"\n\n# The tokenizer defined here will be used later to generate responses from refined model\nllama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True, do_sample=True)\nllama_tokenizer.pad_token = llama_tokenizer.eos_token\nllama_tokenizer.padding_side = \"right\"\nllama_tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n\n# This is used to break the model down to reduce memory usage passed on to the LLM loading function\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False\n)\n\n# Here the LLM is configuration are set\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    quantization_config=quant_config,\n    do_sample=True,\n    device_map=\"auto\"\n)\nbase_model.config.use_cache = False\nbase_model.config.pretraining_tp = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This defines PEFT parameters for fine tuning training\npeft_parameters = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=8,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n#other training arguments taken from web\n#Things like epochs, LR etc. can be adjusted here\ntrain_params = TrainingArguments(\n    output_dir=\"results_modified\",\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\"\n)\n\n#Trainer for fine tuning defined here, provided by HuggingFace\nfine_tuning = SFTTrainer(\n    model=base_model,\n    train_dataset=train_dataset,\n    peft_config=peft_parameters,\n    dataset_text_field=\"Questions\",\n    tokenizer=llama_tokenizer,\n    args=train_params\n)\n\n# Trainer function called\nfine_tuning.train()\n\n# fine tuned model saved\nfine_tuning.model.save_pretrained(refined_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T20:19:20.570879Z","iopub.execute_input":"2024-03-20T20:19:20.571417Z","iopub.status.idle":"2024-03-20T20:22:55.842205Z","shell.execute_reply.started":"2024-03-20T20:19:20.571391Z","shell.execute_reply":"2024-03-20T20:22:55.841121Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/162 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2489822e39c646a8b0b3fbb76ff4c8ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='205' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [205/205 03:32, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.467500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.674000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.456500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.182500</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.046700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.773700</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.709000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.573900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/llama-2/pytorch/7b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/llama-2/pytorch/7b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/llama-2/pytorch/7b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/llama-2/pytorch/7b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/llama-2/pytorch/7b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/llama-2/pytorch/7b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/llama-2/pytorch/7b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/llama-2/pytorch/7b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/llama-2/pytorch/7b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# pipeline from transformer library defined to generate responses using refined model\n# max_length should be adjusted according to data\ntext_gen = pipeline(task=\"text-generation\", model=refined_model, tokenizer=llama_tokenizer, max_length=350)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T20:41:56.794985Z","iopub.execute_input":"2024-03-20T20:41:56.795980Z","iopub.status.idle":"2024-03-20T20:43:36.069709Z","shell.execute_reply.started":"2024-03-20T20:41:56.795934Z","shell.execute_reply":"2024-03-20T20:43:36.068762Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5628f38dc2245acb6aa5ed99212e37b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Sample tested from test data\n# test_data[\"Questions\"].iloc[40] is a query of string data type\noutput = text_gen(test_data[\"Questions\"].iloc[40]) # any other question string can be passed\nprint(output[0]['generated_text']) #output printed","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:05:00.810639Z","iopub.execute_input":"2024-03-20T21:05:00.811089Z","iopub.status.idle":"2024-03-20T21:09:28.471968Z","shell.execute_reply.started":"2024-03-20T21:05:00.811049Z","shell.execute_reply":"2024-03-20T21:09:28.470877Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"What are the common symptoms of type 1 diabetes in children, and how does it differ from type 2 diabetes?\n\nType 1 diabetes is an autoimmune disease in which the immune system attacks and destroys the cells in the pancreas that produce insulin, a hormone that regulates blood sugar levels. This results in a complete deficiency of insulin production and a reliance on insulin injections or an insulin pump to manage blood sugar levels.\n\nThe common symptoms of type 1 diabetes in children include:\n\n1. Increased thirst and urination: When there is too much glucose in the blood, the body tries to flush it out by producing more urine, leading to increased thirst and frequent urination.\n2. Fatigue: High blood sugar levels can cause fatigue, lethargy, and a lack of energy.\n3. Weight loss: Despite increased thirst and urination, children with type 1 diabetes may experience weight loss due to the body's inability to use glucose for energy.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# This line is used to generate a text file containing list of all the libraries used along with their version number\n# !pip freeze > requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-03-20T14:10:26.825660Z","iopub.execute_input":"2024-03-20T14:10:26.825961Z","iopub.status.idle":"2024-03-20T14:10:26.851561Z","shell.execute_reply.started":"2024-03-20T14:10:26.825912Z","shell.execute_reply":"2024-03-20T14:10:26.850692Z"},"trusted":true},"execution_count":11,"outputs":[]}]}