{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7865634,"sourceType":"datasetVersion","datasetId":4614563},{"sourceId":4298,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3093}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Note: It is important to specify the versions of libraries used after getting the code to run first time. These libraries are updated quite often and this usually results in error and incompatibilty between liberaries.\n\nUse:\npip freeze > requirements.txt\nto generate a file containing all the details about libraries used and their versions. This will come handy in case of replicating code.\n\nOn kaggle keep the environment pinned to original to avoid conflict in future.","metadata":{}},{"cell_type":"code","source":"!pip install -q langchain==0.1.6\n!pip install -q chromadb","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:18:46.294956Z","iopub.execute_input":"2024-03-21T11:18:46.295306Z","iopub.status.idle":"2024-03-21T11:19:46.867574Z","shell.execute_reply.started":"2024-03-21T11:18:46.295276Z","shell.execute_reply":"2024-03-21T11:19:46.866491Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q torch bitsandbytes transformers==4.21.3 accelerate","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:19:46.869003Z","iopub.execute_input":"2024-03-21T11:19:46.869284Z","iopub.status.idle":"2024-03-21T11:20:13.488050Z","shell.execute_reply.started":"2024-03-21T11:19:46.869260Z","shell.execute_reply":"2024-03-21T11:20:13.486882Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nchromadb 0.4.24 requires tokenizers>=0.13.2, but you have tokenizers 0.12.1 which is incompatible.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.21.3 which is incompatible.\nllama-index-embeddings-huggingface 0.1.4 requires transformers<5.0.0,>=4.37.0, but you have transformers 4.21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q pypdf\n!pip install -q sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:20:13.491579Z","iopub.execute_input":"2024-03-21T11:20:13.491973Z","iopub.status.idle":"2024-03-21T11:20:51.508288Z","shell.execute_reply.started":"2024-03-21T11:20:13.491939Z","shell.execute_reply":"2024-03-21T11:20:51.506962Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#loading documents from folder containing the pdfs\nfrom langchain.document_loaders import PyPDFDirectoryLoader\nloader = PyPDFDirectoryLoader(\"/kaggle/input/diabetes-documents\")\ndocs = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:20:51.529314Z","iopub.execute_input":"2024-03-21T11:20:51.529610Z","iopub.status.idle":"2024-03-21T11:22:03.749525Z","shell.execute_reply.started":"2024-03-21T11:20:51.529586Z","shell.execute_reply":"2024-03-21T11:22:03.748557Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#splitting text to make embeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\ndocs = text_splitter.split_documents(docs)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:22:03.751488Z","iopub.execute_input":"2024-03-21T11:22:03.752135Z","iopub.status.idle":"2024-03-21T11:22:03.870216Z","shell.execute_reply.started":"2024-03-21T11:22:03.752097Z","shell.execute_reply":"2024-03-21T11:22:03.869403Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#defining embedding function\n#sentence transformer model used to save memory\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\nmodel_kwargs = {'device':'cuda'}\nencode_kwargs = {'normalize_embeddings':False}\nembeddings_func = HuggingFaceEmbeddings(\n  model_name = 'sentence-transformers/all-mpnet-base-v2',  \n  model_kwargs = model_kwargs,\n  encode_kwargs=encode_kwargs\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:22:03.871642Z","iopub.execute_input":"2024-03-21T11:22:03.872050Z","iopub.status.idle":"2024-03-21T11:22:27.070952Z","shell.execute_reply.started":"2024-03-21T11:22:03.872014Z","shell.execute_reply":"2024-03-21T11:22:27.070073Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a781f46c8cdd49119a9c5a04a076aba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75004b261d314801bac34a2f6af5971e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3aad76e58dd47bd8c729bcc5b84642c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a017790cff904c9fa9c5b703db615d73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84b576fd92cf49f7adf24d375bc678bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63328481c97d4c4fa3d2eb8acd488b51"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1231d172da904ad3863cf1bf34dffaa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bead9410eb84051bf2b1b09d97134a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b86215306e4a4829b6812dcf05657ab3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66510da119804dd6a50001f14c06663f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ddc7f615e2a462f96d5f046d281d44c"}},"metadata":{}}]},{"cell_type":"code","source":"#Create a vector store for embeddings in local storage using chromaDB\nfrom langchain.vectorstores import Chroma\n\n#for creating the vector store we pass chunks of docs and embedding function\n#These two lines are used to create the store and used only once or when docs are changed\n#Comment out these lines if vector store already created\ndb=Chroma.from_documents(docs, embeddings_func, persist_directory=\"chroma_db\")\ndb.persist()\n\n#The following line is used to load Vector store already created\n# db = Chroma(persist_directory=\"/kaggle/working/chroma_db\", embedding_function=embeddings_func)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:22:27.080195Z","iopub.execute_input":"2024-03-21T11:22:27.080577Z","iopub.status.idle":"2024-03-21T11:22:28.025848Z","shell.execute_reply.started":"2024-03-21T11:22:27.080544Z","shell.execute_reply":"2024-03-21T11:22:28.024941Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#This code contains standard loading procedure for a LLM using hugging face libraries. \n#llama-2 7b-chat is used for both tokenization and generation\n#To access llama-2 in kaggle go to +Add input -> Models -> llama-2\n#Copy it's path here as base_model_name\n\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    pipeline\n)\nfrom langchain import HuggingFacePipeline\n\nbase_model_name = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name, max_length=512, padding=True, truncation=True)\n\n\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_name)\nbase_model.config.use_cache = False\n\n# pipeline is create to use the LLM to generate responses\n# It integrates tokenization and generation for us\n\npipe = pipeline(\"text2text-generation\", model=base_model, tokenizer=tokenizer)\nllm = HuggingFacePipeline(pipeline = pipe)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:22:28.027143Z","iopub.execute_input":"2024-03-21T11:22:28.027508Z","iopub.status.idle":"2024-03-21T11:24:17.293447Z","shell.execute_reply.started":"2024-03-21T11:22:28.027481Z","shell.execute_reply":"2024-03-21T11:24:17.291654Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2024-03-21 11:22:29.836045: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-21 11:22:29.836169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-21 11:22:29.970173: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf43ab51175a4a299fc7b4110af21a9a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nThe model 'LlamaForCausalLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n","output_type":"stream"}]},{"cell_type":"code","source":"#print information about pipeline\n# print(llm)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:24:17.296868Z","iopub.execute_input":"2024-03-21T11:24:17.297612Z","iopub.status.idle":"2024-03-21T11:24:17.304356Z","shell.execute_reply.started":"2024-03-21T11:24:17.297577Z","shell.execute_reply":"2024-03-21T11:24:17.303218Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#define a template for augmented prompt sent to the LLM\n#The {context} and {question} are added by Retrieval Model itself\n\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\nQA_CHAIN_PROMPT = PromptTemplate.from_template(template)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:24:17.306064Z","iopub.execute_input":"2024-03-21T11:24:17.306448Z","iopub.status.idle":"2024-03-21T11:24:17.482833Z","shell.execute_reply.started":"2024-03-21T11:24:17.306412Z","shell.execute_reply":"2024-03-21T11:24:17.481788Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# RetrievalQA by langchain lets us augment the prompt before passing it to HuggingFacePipeline \n\nfrom langchain.chains import RetrievalQA \n\nqa_chain = RetrievalQA.from_chain_type(\n    llm, retriever=db.as_retriever(), chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n)\n\nquestion = \"How can language barriers be addressed in diabetes care?\"\nresult = qa_chain({\"query\": question})\nprint(result[\"result\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:24:17.484233Z","iopub.execute_input":"2024-03-21T11:24:17.484570Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"}]}]}